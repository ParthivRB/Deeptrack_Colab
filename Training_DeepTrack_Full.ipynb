```python
# === SECTION 1: Setup ===
! pip install deeptrack

from google.colab import drive
drive.mount('/content/drive')

import deeptrack as dt
import torch
import numpy as np
from pathlib import Path

# === SECTION 2: Configuration ===
CONFIG = {
    'data_path': '/content/drive/MyDrive/DeepTrack_Training/videos',
    'output_path': '/content/drive/MyDrive/DeepTrack_Models',
    'model_name': 'particle_detector_v1',
    
    # Training params
    'architecture': 'unet',  # 'unet', 'lodestar', 'custom'
    'input_size': (512, 512),
    'batch_size': 4,
    'epochs': 50,
    'learning_rate':  1e-4,
    
    # DeepTrack features
    'simulate_data': True,  # Generate synthetic data
    'augmentation': True,
    'particle_size_range': (50, 200),  # nm
}

# === SECTION 3: Data Loading ===
def load_training_data(config):
    """Load videos and annotations"""
    video_paths = list(Path(config['data_path']).glob('*.tif'))
    
    frames = []
    labels = []
    
    for video_path in video_paths: 
        # Load video
        video = dt. sources. Folder(str(video_path))
        
        # Load annotations (if available)
        annotation_path = video_path.parent / 'annotations' / f"{video_path.stem}_particles.csv"
        if annotation_path.exists():
            annotations = pd.read_csv(annotation_path)
            # Convert to masks
            labels.append(create_masks_from_annotations(annotations))
        
        frames.append(video)
    
    return frames, labels

# === SECTION 4: Synthetic Data Generation ===
def create_synthetic_data(config, num_samples=1000):
    """Generate training data using DeepTrack2 simulation"""
    
    # Define particle
    particle = dt. Sphere(
        radius=dt.uniform(50, 200),  # nm
        refractive_index=1.45,
        position=dt.uniform((0, 0), (512, 512))
    )
    
    # Define optics
    optics = dt.Brightfield(
        magnification=60,
        NA=1.4,
        wavelength=532,  # nm
        resolution=(512, 512),
        upscale=1
    )
    
    # Create pipeline
    pipeline = optics(particle)
    
    # Add noise and augmentation
    pipeline = dt. Gaussian(sigma=0.01)(pipeline)
    
    if config['augmentation']:
        pipeline = dt. FlipLR()(pipeline)
        pipeline = dt.FlipUD()(pipeline)
        pipeline = dt.Rotate(angle=dt.uniform(0, 360))(pipeline)
    
    # Generate dataset
    dataset = []
    for i in range(num_samples):
        image = pipeline()
        label = create_label_from_properties(pipeline.properties)
        dataset.append((image, label))
    
    return dataset

# === SECTION 5: Model Training ===
def train_model(config):
    """Train DeepTrack model"""
    
    # Load or generate data
    if config['simulate_data']:
        print("Generating synthetic training data...")
        dataset = create_synthetic_data(config)
    else:
        print("Loading real training data...")
        dataset = load_training_data(config)
    
    # Create model
    if config['architecture'] == 'unet':
        model = dt. models.UNet(
            input_shape=(*config['input_size'], 1),
            conv_layers_dimensions=[16, 32, 64, 128],
            dense_layers_dimensions=[32],
            number_of_outputs=1,
            output_activation='sigmoid'
        )
    elif config['architecture'] == 'lodestar':
        model = dt.models.LodeSTAR(
            input_shape=(*config['input_size'], 1)
        )
    
    # Training loop
    model.compile(
        optimizer=torch.optim.Adam(model. parameters(), lr=config['learning_rate']),
        loss=torch.nn.BCEWithLogitsLoss()
    )
    
    print("Starting training...")
    for epoch in range(config['epochs']):
        epoch_loss = 0
        for batch in dataloader:
            loss = model.train_step(batch)
            epoch_loss += loss
        
        print(f"Epoch {epoch+1}/{config['epochs']}, Loss: {epoch_loss/len(dataloader):.4f}")
        
        # Save checkpoint
        if (epoch + 1) % 10 == 0:
            save_checkpoint(model, config, epoch)
    
    # Save final model
    save_final_model(model, config)
    print(f"Model saved to:  {config['output_path']}/{config['model_name']}.pth")

# === SECTION 6: Model Export ===
def save_final_model(model, config):
    """Save model with metadata"""
    output_dir = Path(config['output_path'])
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Save weights
    model_path = output_dir / f"{config['model_name']}.pth"
    torch.save(model.state_dict(), model_path)
    
    # Save metadata
    metadata = {
        'architecture': config['architecture'],
        'input_size': config['input_size'],
        'trained_on': str(datetime.now()),
        'config': config
    }
    
    metadata_path = output_dir / f"{config['model_name']}_metadata.json"
    with open(metadata_path, 'w') as f:
        json. dump(metadata, f, indent=2)

# === SECTION 7: Run Training ===
if __name__ == '__main__':
    train_model(CONFIG)
```