{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ DeepTrack2 Cloud Training System\n",
    "## Complete Training Pipeline for Particle Tracking Models\n",
    "\n",
    "**Version:** 1.0.0  \n",
    "**Author:** DeepTrack MPT Studio  \n",
    "**Last Updated:** 2025-12-19\n",
    "\n",
    "### Features:\n",
    "- âœ… Multiple architectures (UNet, LodeSTAR)\n",
    "- âœ… Automatic data loading from Google Drive\n",
    "- âœ… Interactive configuration\n",
    "- âœ… Progress tracking & visualization\n",
    "- âœ… Model versioning & metadata\n",
    "- âœ… One-click export for local use"
   ]
  },
  {
   "cell_type":  "markdown",
   "metadata":  {},
   "source": [
    "---\n",
    "## ðŸ”§ SECTION 1: Environment Setup"
   ]
  },
  {
   "cell_type":  "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install dependencies (suppress output for cleaner notebook)\n",
    "!pip install deeptrack deeplay torch torchvision tqdm ipywidgets matplotlib scikit-image pandas scipy"
   ]
  },
  {
   "cell_type":  "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import shutil\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "from skimage import io as skio\n",
    "from tqdm. auto import tqdm\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn. functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import deeplay as dl\n",
    "import deeptrack as dt\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸ–¥ï¸  Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU:  {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\nðŸ“š Library Versions:\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   DeepTrack: {dt.__version__}\")\n",
    "print(f\"   DeepLay: {dl.__version__}\")"
   ]
  },
  {
   "cell_type":  "code",
   "execution_count":  null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# Define base paths\n",
    "BASE_PATH = Path('/content/drive/MyDrive/DeepTrack_Studio')\n",
    "DATA_PATH = BASE_PATH / 'training_data'\n",
    "MODEL_PATH = BASE_PATH / 'models'\n",
    "LOG_PATH = BASE_PATH / 'logs'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for path in [BASE_PATH, DATA_PATH, MODEL_PATH, LOG_PATH]:\n",
    "    path. mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(f\"âœ… Google Drive mounted successfully! \")\n",
    "print(f\"ðŸ“ Base directory: {BASE_PATH}\")\n",
    "print(f\"\\nðŸ“‚ Directory Structure:\")\n",
    "print(f\"   Training Data: {DATA_PATH}\")\n",
    "print(f\"   Models: {MODEL_PATH}\")\n",
    "print(f\"   Logs:  {LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“Š SECTION 2: Data Management System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs":  [],
   "source": [
    "class TrainingDataLoader:\n",
    "    \"\"\"\n",
    "    Handles loading and validation of training data from Google Drive.\n",
    "    \n",
    "    Expected structure:\n",
    "    training_data/\n",
    "        videos/\n",
    "            video_001.tif\n",
    "            video_002.tif\n",
    "        annotations/\n",
    "            video_001_particles.csv\n",
    "            video_002_particles.csv\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.videos_path = self.data_path / 'videos'\n",
    "        self.annotations_path = self.data_path / 'annotations'\n",
    "        \n",
    "        # Create directories if missing\n",
    "        self.videos_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.annotations_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.video_files = []\n",
    "        self.annotation_files = {}\n",
    "        \n",
    "    def scan_data(self):\n",
    "        \"\"\"Scan for available videos and annotations.\"\"\"\n",
    "        print(\"ðŸ” Scanning for training data...\")\n",
    "        \n",
    "        # Find all video files\n",
    "        video_extensions = ['.tif', '.tiff', '.png', '.jpg']\n",
    "        self.video_files = []\n",
    "        for ext in video_extensions:\n",
    "            self.video_files.extend(list(self.videos_path.glob(f\"*{ext}\")))\n",
    "        \n",
    "        if not self.video_files:\n",
    "            print(f\"âŒ No videos found in {self.videos_path}\")\n",
    "            print(f\"\\nðŸ“ Upload Instructions:\")\n",
    "            print(f\"   1. Open Google Drive\")\n",
    "            print(f\"   2. Navigate to:  {self.videos_path}\")\n",
    "            print(f\"   3. Upload your . tif video files\")\n",
    "            print(f\"   4. (Optional) Upload annotations to:  {self.annotations_path}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"âœ… Found {len(self. video_files)} video file(s)\")\n",
    "        \n",
    "        # Match annotations\n",
    "        self. annotation_files = {}\n",
    "        for video_path in self.video_files:\n",
    "            annotation_path = self.annotations_path / f\"{video_path. stem}_particles.csv\"\n",
    "            if annotation_path. exists():\n",
    "                self.annotation_files[video_path.stem] = annotation_path\n",
    "        \n",
    "        print(f\"ðŸ“‹ Found {len(self.annotation_files)} annotation file(s)\")\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"\\nðŸ“ Data Summary:\")\n",
    "        for i, video_path in enumerate(self.video_files, 1):\n",
    "            has_annotation = video_path.stem in self.annotation_files\n",
    "            status = \"âœ…\" if has_annotation else \"âš ï¸ (no annotation)\"\n",
    "            print(f\"   {i}. {video_path.name} {status}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def load_video(self, video_path):\n",
    "        \"\"\"Load a video file.\"\"\"\n",
    "        video = skio.imread(str(video_path))\n",
    "        \n",
    "        # Handle different dimensions\n",
    "        if video.ndim == 2:\n",
    "            video = video[np.newaxis, ...]  # Add time dimension\n",
    "        elif video.ndim == 4:\n",
    "            video = video[:, 0, :, :]  # Take first channel if multi-channel\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        if video.max() > 0:\n",
    "            video = video. astype(np.float32) / video.max()\n",
    "        \n",
    "        return video\n",
    "    \n",
    "    def load_annotations(self, video_stem):\n",
    "        \"\"\"Load annotations for a video.\"\"\"\n",
    "        if video_stem not in self.annotation_files:\n",
    "            return None\n",
    "        \n",
    "        df = pd.read_csv(self.annotation_files[video_stem])\n",
    "        \n",
    "        # Validate required columns\n",
    "        required_cols = ['frame', 'x', 'y']\n",
    "        if not all(col in df.columns for col in required_cols):\n",
    "            raise ValueError(f\"Annotation file must contain columns: {required_cols}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_ground_truth_masks(self, annotations, shape, radius=3):\n",
    "        \"\"\"\n",
    "        Create binary masks from annotations.\n",
    "        \n",
    "        Args:\n",
    "            annotations: DataFrame with columns [frame, x, y]\n",
    "            shape: (num_frames, height, width)\n",
    "            radius: Radius of particles in pixels\n",
    "        \"\"\"\n",
    "        num_frames, height, width = shape\n",
    "        masks = np.zeros(shape, dtype=np.float32)\n",
    "        \n",
    "        # Create coordinate grids\n",
    "        yy, xx = np.ogrid[: height, :width]\n",
    "        \n",
    "        for frame_idx in range(num_frames):\n",
    "            frame_particles = annotations[annotations['frame'] == frame_idx]\n",
    "            \n",
    "            for _, particle in frame_particles.iterrows():\n",
    "                x, y = particle['x'], particle['y']\n",
    "                \n",
    "                # Create circular mask\n",
    "                distance = (xx - x)**2 + (yy - y)**2\n",
    "                masks[frame_idx][distance <= radius**2] = 1.0\n",
    "        \n",
    "        return masks\n",
    "    \n",
    "    def preview_data(self, video_idx=0, frame_idx=0):\n",
    "        \"\"\"Preview a video frame with annotations overlay.\"\"\"\n",
    "        if not self.video_files:\n",
    "            print(\"âŒ No videos available\")\n",
    "            return\n",
    "        \n",
    "        video_path = self.video_files[video_idx]\n",
    "        video = self.load_video(video_path)\n",
    "        \n",
    "        annotations = self.load_annotations(video_path.stem)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Original frame\n",
    "        axes[0].imshow(video[frame_idx], cmap='gray')\n",
    "        axes[0].set_title(f\"{video_path.name} - Frame {frame_idx}\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Frame with annotations\n",
    "        axes[1].imshow(video[frame_idx], cmap='gray')\n",
    "        if annotations is not None:\n",
    "            frame_particles = annotations[annotations['frame'] == frame_idx]\n",
    "            if not frame_particles.empty:\n",
    "                axes[1].scatter(frame_particles['x'], frame_particles['y'], \n",
    "                              c='red', s=50, marker='o', facecolors='none', linewidths=2)\n",
    "        axes[1].set_title(f\"With Annotations ({len(frame_particles) if annotations is not None else 0} particles)\")\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt. tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Video Info:\")\n",
    "        print(f\"   Shape: {video.shape}\")\n",
    "        print(f\"   Frames: {len(video)}\")\n",
    "        print(f\"   Data type: {video.dtype}\")\n",
    "        print(f\"   Value range: [{video.min():.3f}, {video.max():.3f}]\")\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = TrainingDataLoader(DATA_PATH)\n",
    "data_available = data_loader.scan_data()"
   ]
  },
  {
   "cell_type":  "code",
   "execution_count":  null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview loaded data\n",
    "if data_available:\n",
    "    data_loader.preview_data(video_idx=0, frame_idx=0)"
   ]
  },
  {
   "cell_type":  "markdown",
   "metadata":  {},
   "source": [
    "---\n",
    "## âš™ï¸ SECTION 3: Interactive Configuration System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs":  [],
   "source": [
    "class TrainingConfig:\n",
    "    \"\"\"Configuration management with interactive widgets.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self. config = {}\n",
    "        self.widgets = {}\n",
    "        self._create_widgets()\n",
    "    \n",
    "    def _create_widgets(self):\n",
    "        \"\"\"Create all configuration widgets.\"\"\"\n",
    "        \n",
    "        # === MODEL ARCHITECTURE ===\n",
    "        self.widgets['architecture'] = widgets. Dropdown(\n",
    "            options=['UNet', 'LodeSTAR'],\n",
    "            value='UNet',\n",
    "            description='Architecture:',\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        self.widgets['model_name'] = widgets.Text(\n",
    "            value='particle_detector',\n",
    "            description='Model Name:',\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        # UNet specific\n",
    "        self.widgets['unet_channels'] = widgets.Text(\n",
    "            value='16,32,64',\n",
    "            description='UNet Channels:',\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        # === TRAINING PARAMETERS ===\n",
    "        self.widgets['epochs'] = widgets.IntSlider(\n",
    "            value=50,\n",
    "            min=10,\n",
    "            max=200,\n",
    "            step=10,\n",
    "            description='Epochs: ',\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='500px')\n",
    "        )\n",
    "        \n",
    "        self.widgets['batch_size'] = widgets. Dropdown(\n",
    "            options=[2, 4, 8, 16],\n",
    "            value=4,\n",
    "            description='Batch Size: ',\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        self.widgets['learning_rate'] = widgets.FloatLogSlider(\n",
    "            value=1e-4,\n",
    "            base=10,\n",
    "            min=-6,\n",
    "            max=-2,\n",
    "            step=0.1,\n",
    "            description='Learning Rate:',\n",
    "            readout_format='.2e',\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='500px')\n",
    "        )\n",
    "        \n",
    "        self.widgets['validation_split'] = widgets.FloatSlider(\n",
    "            value=0.2,\n",
    "            min=0.1,\n",
    "            max=0.4,\n",
    "            step=0.05,\n",
    "            description='Validation Split:',\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='500px')\n",
    "        )\n",
    "        \n",
    "        # === AUGMENTATION ===\n",
    "        self.widgets['augmentation'] = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Enable Augmentation',\n",
    "            style={'description_width': '150px'}\n",
    "        )\n",
    "        \n",
    "        self.widgets['flip_lr'] = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Flip Left-Right',\n",
    "            indent=True\n",
    "        )\n",
    "        \n",
    "        self.widgets['flip_ud'] = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Flip Up-Down',\n",
    "            indent=True\n",
    "        )\n",
    "        \n",
    "        self.widgets['rotate'] = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Random Rotation',\n",
    "            indent=True\n",
    "        )\n",
    "        \n",
    "        self.widgets['brightness'] = widgets. Checkbox(\n",
    "            value=True,\n",
    "            description='Brightness Variation',\n",
    "            indent=True\n",
    "        )\n",
    "        \n",
    "        # === DATA PARAMETERS ===\n",
    "        self.widgets['particle_radius'] = widgets.IntSlider(\n",
    "            value=3,\n",
    "            min=1,\n",
    "            max=10,\n",
    "            description='Particle Radius (px):',\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='500px')\n",
    "        )\n",
    "        \n",
    "        # === SYNTHETIC DATA ===\n",
    "        self.widgets['use_synthetic'] = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Generate Synthetic Data',\n",
    "            style={'description_width': '150px'}\n",
    "        )\n",
    "        \n",
    "        self.widgets['synthetic_samples'] = widgets.IntText(\n",
    "            value=500,\n",
    "            description='Synthetic Samples:',\n",
    "            style={'description_width': '150px'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display configuration interface.\"\"\"\n",
    "        display(HTML(\"<h3>ðŸ”§ Training Configuration</h3>\"))\n",
    "        \n",
    "        # Model section\n",
    "        display(HTML(\"<h4>ðŸ“¦ Model Architecture</h4>\"))\n",
    "        display(widgets. VBox([\n",
    "            self.widgets['model_name'],\n",
    "            self.widgets['architecture'],\n",
    "            self.widgets['unet_channels']\n",
    "        ]))\n",
    "        \n",
    "        # Training section\n",
    "        display(HTML(\"<h4>ðŸŽ¯ Training Parameters</h4>\"))\n",
    "        display(widgets.VBox([\n",
    "            self.widgets['epochs'],\n",
    "            self.widgets['batch_size'],\n",
    "            self.widgets['learning_rate'],\n",
    "            self.widgets['validation_split']\n",
    "        ]))\n",
    "        \n",
    "        # Augmentation section\n",
    "        display(HTML(\"<h4>ðŸ”„ Data Augmentation</h4>\"))\n",
    "        display(widgets.VBox([\n",
    "            self.widgets['augmentation'],\n",
    "            widgets.HBox([\n",
    "                self.widgets['flip_lr'],\n",
    "                self. widgets['flip_ud'],\n",
    "                self.widgets['rotate'],\n",
    "                self.widgets['brightness']\n",
    "            ])\n",
    "        ]))\n",
    "        \n",
    "        # Data section\n",
    "        display(HTML(\"<h4>ðŸ“Š Data Parameters</h4>\"))\n",
    "        display(widgets.VBox([\n",
    "            self.widgets['particle_radius']\n",
    "        ]))\n",
    "        \n",
    "        # Synthetic data section\n",
    "        display(HTML(\"<h4>ðŸŽ¨ Synthetic Data (Optional)</h4>\"))\n",
    "        display(widgets.VBox([\n",
    "            self.widgets['use_synthetic'],\n",
    "            self.widgets['synthetic_samples']\n",
    "        ]))\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Extract configuration from widgets.\"\"\"\n",
    "        config = {\n",
    "            'model':  {\n",
    "                'name': self.widgets['model_name'].value,\n",
    "                'architecture': self. widgets['architecture'].value. lower(),\n",
    "                'unet_channels': [int(x. strip()) for x in self.widgets['unet_channels'].value. split(',')]\n",
    "            },\n",
    "            'training': {\n",
    "                'epochs': self.widgets['epochs'].value,\n",
    "                'batch_size':  self.widgets['batch_size'].value,\n",
    "                'learning_rate': self.widgets['learning_rate'].value,\n",
    "                'validation_split': self.widgets['validation_split'].value\n",
    "            },\n",
    "            'augmentation': {\n",
    "                'enabled': self.widgets['augmentation'].value,\n",
    "                'flip_lr': self.widgets['flip_lr'].value,\n",
    "                'flip_ud': self.widgets['flip_ud'].value,\n",
    "                'rotate': self.widgets['rotate'].value,\n",
    "                'brightness': self.widgets['brightness'].value\n",
    "            },\n",
    "            'data': {\n",
    "                'particle_radius': self.widgets['particle_radius'].value,\n",
    "                'use_synthetic': self.widgets['use_synthetic'].value,\n",
    "                'synthetic_samples': self.widgets['synthetic_samples'].value\n",
    "            }\n",
    "        }\n",
    "        return config\n",
    "\n",
    "# Create and display configuration\n",
    "config_manager = TrainingConfig()\n",
    "config_manager.display()"
   ]
  },
  {
   "cell_type":  "markdown",
   "metadata":  {},
   "source": [
    "---\n",
    "## ðŸŽ¨ SECTION 4: Data Pipeline & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for particle tracking with augmentation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, frames, masks, augmentation_config=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frames: numpy array of shape (N, H, W)\n",
    "            masks:  numpy array of shape (N, H, W)\n",
    "            augmentation_config: dict with augmentation settings\n",
    "        \"\"\"\n",
    "        self.frames = frames\n",
    "        self.masks = masks\n",
    "        self.aug_config = augmentation_config or {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame = self.frames[idx]. copy()\n",
    "        mask = self.masks[idx].copy()\n",
    "        \n",
    "        # Apply augmentations if enabled\n",
    "        if self.aug_config. get('enabled', False):\n",
    "            frame, mask = self._apply_augmentation(frame, mask)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        frame = torch.from_numpy(frame).float().unsqueeze(0)  # Add channel dimension\n",
    "        mask = torch.from_numpy(mask).float().unsqueeze(0)\n",
    "        \n",
    "        return frame, mask\n",
    "    \n",
    "    def _apply_augmentation(self, frame, mask):\n",
    "        \"\"\"Apply random augmentations to frame and mask.\"\"\"\n",
    "        \n",
    "        # Flip left-right\n",
    "        if self.aug_config. get('flip_lr', False) and np.random.rand() > 0.5:\n",
    "            frame = np.fliplr(frame)\n",
    "            mask = np.fliplr(mask)\n",
    "        \n",
    "        # Flip up-down\n",
    "        if self.aug_config.get('flip_ud', False) and np.random.rand() > 0.5:\n",
    "            frame = np.flipud(frame)\n",
    "            mask = np.flipud(mask)\n",
    "        \n",
    "        # Random rotation (90, 180, 270 degrees)\n",
    "        if self.aug_config.get('rotate', False) and np.random.rand() > 0.5:\n",
    "            k = np.random.randint(1, 4)\n",
    "            frame = np.rot90(frame, k)\n",
    "            mask = np.rot90(mask, k)\n",
    "        \n",
    "        # Brightness variation\n",
    "        if self.aug_config. get('brightness', False) and np.random.rand() > 0.5:\n",
    "            factor = np.random.uniform(0.8, 1.2)\n",
    "            frame = np.clip(frame * factor, 0, 1)\n",
    "        \n",
    "        return frame, mask\n",
    "\n",
    "\n",
    "def prepare_datasets(data_loader, config):\n",
    "    \"\"\"\n",
    "    Prepare training and validation datasets.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“¦ Preparing datasets...\")\n",
    "    \n",
    "    all_frames = []\n",
    "    all_masks = []\n",
    "    \n",
    "    # Load all videos and create masks\n",
    "    for video_path in tqdm(data_loader. video_files, desc=\"Loading videos\"):\n",
    "        # Load video\n",
    "        video = data_loader.load_video(video_path)\n",
    "        \n",
    "        # Load or create masks\n",
    "        annotations = data_loader.load_annotations(video_path. stem)\n",
    "        if annotations is not None:\n",
    "            masks = data_loader.create_ground_truth_masks(\n",
    "                annotations, \n",
    "                video. shape,\n",
    "                radius=config['data']['particle_radius']\n",
    "            )\n",
    "        else:\n",
    "            # No annotations - unsupervised mode (masks are zeros)\n",
    "            masks = np.zeros_like(video)\n",
    "        \n",
    "        all_frames.append(video)\n",
    "        all_masks.append(masks)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    all_frames = np.concatenate(all_frames, axis=0)\n",
    "    all_masks = np. concatenate(all_masks, axis=0)\n",
    "    \n",
    "    print(f\"âœ… Total frames: {len(all_frames)}\")\n",
    "    \n",
    "    # Split into train/validation\n",
    "    val_split = config['training']['validation_split']\n",
    "    n_val = int(len(all_frames) * val_split)\n",
    "    \n",
    "    indices = np.random.permutation(len(all_frames))\n",
    "    train_indices = indices[n_val:]\n",
    "    val_indices = indices[: n_val]\n",
    "    \n",
    "    train_frames = all_frames[train_indices]\n",
    "    train_masks = all_masks[train_indices]\n",
    "    val_frames = all_frames[val_indices]\n",
    "    val_masks = all_masks[val_indices]\n",
    "    \n",
    "    print(f\"ðŸ“Š Train samples: {len(train_frames)}\")\n",
    "    print(f\"ðŸ“Š Validation samples: {len(val_frames)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ParticleDataset(train_frames, train_masks, config['augmentation'])\n",
    "    val_dataset = ParticleDataset(val_frames, val_masks, None)  # No augmentation for validation\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "print(\"âœ… Data pipeline ready!\")"
   ]
  },
  {
   "cell_type":  "markdown",
   "metadata":  {},
   "source": [
    "---\n",
    "## ðŸ—ï¸ SECTION 5: Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs":  [],
   "source": [
    "def create_model(config):\n",
    "    \"\"\"\n",
    "    Create model based on configuration.\n",
    "    \"\"\"\n",
    "    arch_type = config['model']['architecture']\n",
    "    \n",
    "    print(f\"ðŸ—ï¸  Building {arch_type. upper()} model...\")\n",
    "    \n",
    "    if arch_type == 'unet':\n",
    "        model = dl.UNet2d(\n",
    "            in_channels=1,\n",
    "            channels=config['model']['unet_channels'],\n",
    "            out_channels=1\n",
    "        )\n",
    "    elif arch_type == 'lodestar':\n",
    "        # LodeSTAR implementation (simplified for now)\n",
    "        print(\"âš ï¸  LodeSTAR is experimental - using UNet architecture\")\n",
    "        model = dl.UNet2d(\n",
    "            in_channels=1,\n",
    "            channels=[32, 64, 128],\n",
    "            out_channels=1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown architecture: {arch_type}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p. numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"âœ… Model created successfully! \")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"âœ… Model builder ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸš€ SECTION 6: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs":  [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Handles model training with progress tracking and checkpointing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, config, save_dir):\n",
    "        self.model = model. to(device)\n",
    "        self.config = config\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Training components\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            model. parameters(),\n",
    "            lr=config['training']['learning_rate']\n",
    "        )\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Tracking\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_iou': [],\n",
    "            'val_iou':  []\n",
    "        }\n",
    "        self.best_val_loss = float('inf')\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_iou = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=\"Training\")\n",
    "        for frames, masks in pbar:\n",
    "            frames = frames. to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(frames)\n",
    "            loss = self. criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer. step()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            with torch.no_grad():\n",
    "                iou = self._calculate_iou(torch.sigmoid(outputs), masks)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_iou += iou\n",
    "            \n",
    "            pbar. set_postfix({'loss': f\"{loss.item():.4f}\", 'iou': f\"{iou:.4f}\"})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_iou = total_iou / len(train_loader)\n",
    "        \n",
    "        return avg_loss, avg_iou\n",
    "    \n",
    "    def validate(self, val_loader):\n",
    "        \"\"\"Validate the model.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_iou = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for frames, masks in tqdm(val_loader, desc=\"Validation\"):\n",
    "                frames = frames.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                outputs = self.model(frames)\n",
    "                loss = self. criterion(outputs, masks)\n",
    "                iou = self._calculate_iou(torch.sigmoid(outputs), masks)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_iou += iou\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        avg_iou = total_iou / len(val_loader)\n",
    "        \n",
    "        return avg_loss, avg_iou\n",
    "    \n",
    "    def _calculate_iou(self, pred, target, threshold=0.5):\n",
    "        \"\"\"Calculate Intersection over Union.\"\"\"\n",
    "        pred_binary = (pred > threshold).float()\n",
    "        target_binary = (target > threshold).float()\n",
    "        \n",
    "        intersection = (pred_binary * target_binary).sum()\n",
    "        union = pred_binary.sum() + target_binary.sum() - intersection\n",
    "        \n",
    "        if union == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        return (intersection / union).item()\n",
    "    \n",
    "    def save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'history': self.history,\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        checkpoint_path = self.save_dir / f\"checkpoint_epoch{epoch}.pth\"\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        \n",
    "        # Save best model\n",
    "        if is_best:\n",
    "            best_path = self.save_dir / \"best_model.pth\"\n",
    "            torch. save(checkpoint, best_path)\n",
    "            print(f\"   ðŸ’¾ Saved best model (val_loss: {self.best_val_loss:.4f})\")\n",
    "    \n",
    "    def plot_progress(self):\n",
    "        \"\"\"Plot training progress.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[0].plot(self. history['train_loss'], label='Train Loss', marker='o')\n",
    "        axes[0].plot(self.history['val_loss'], label='Val Loss', marker='s')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0]. set_title('Training & Validation Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True)\n",
    "        \n",
    "        # IoU plot\n",
    "        axes[1].plot(self. history['train_iou'], label='Train IoU', marker='o')\n",
    "        axes[1].plot(self. history['val_iou'], label='Val IoU', marker='s')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('IoU')\n",
    "        axes[1]. set_title('Training & Validation IoU')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.save_dir / 'training_progress.png', dpi=150, bbox_inches='tight')\n",
    "        plt. show()\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs):\n",
    "        \"\"\"Main training loop.\"\"\"\n",
    "        print(f\"\\nðŸš€ Starting training for {epochs} epochs...\\n\")\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Epoch {epoch}/{epochs}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_iou = self.train_epoch(train_loader)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_iou = self.validate(val_loader)\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_iou']. append(train_iou)\n",
    "            self.history['val_iou'].append(val_iou)\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\nðŸ“Š Epoch {epoch} Summary:\")\n",
    "            print(f\"   Train Loss: {train_loss:.4f} | Train IoU: {train_iou:.4f}\")\n",
    "            print(f\"   Val Loss:    {val_loss:.4f} | Val IoU:   {val_iou:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            is_best = val_loss < self.best_val_loss\n",
    "            if is_best:\n",
    "                self.best_val_loss = val_loss\n",
    "            \n",
    "            if epoch % 10 == 0 or is_best:\n",
    "                self.save_checkpoint(epoch, is_best)\n",
    "            \n",
    "            # Plot progress every 5 epochs\n",
    "            if epoch % 5 == 0:\n",
    "                clear_output(wait=True)\n",
    "                self.plot_progress()\n",
    "        \n",
    "        print(f\"\\nâœ… Training complete! \")\n",
    "        print(f\"   Best validation loss: {self.best_val_loss:.4f}\")\n",
    "        self.plot_progress()\n",
    "\n",
    "print(\"âœ… Trainer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“¦ SECTION 7: Model Export & Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelExporter:\n",
    "    \"\"\"\n",
    "    Handles model export with versioning and metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def generate_version(self):\n",
    "        \"\"\"Generate unique version identifier.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        return f\"v_{timestamp}\"\n",
    "    \n",
    "    def export_model(self, trainer, config, data_info):\n",
    "        \"\"\"\n",
    "        Export trained model with all metadata.\n",
    "        \n",
    "        Args:\n",
    "            trainer:  Trainer instance\n",
    "            config: Training configuration\n",
    "            data_info: Information about training data\n",
    "        \"\"\"\n",
    "        version = self.generate_version()\n",
    "        export_dir = self.model_path / version\n",
    "        export_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nðŸ“¦ Exporting model version:  {version}\")\n",
    "        print(f\"   Directory: {export_dir}\")\n",
    "        \n",
    "        # 1. Save model weights\n",
    "        weights_path = export_dir / \"weights.pth\"\n",
    "        torch.save(trainer.model.state_dict(), weights_path)\n",
    "        print(f\"   âœ… Saved weights:  {weights_path. name}\")\n",
    "        \n",
    "        # 2. Generate metadata\n",
    "        metadata = self._create_metadata(trainer, config, data_info, version)\n",
    "        metadata_path = export_dir / \"metadata.json\"\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        print(f\"   âœ… Saved metadata: {metadata_path.name}\")\n",
    "        \n",
    "        # 3. Save configuration\n",
    "        config_path = export_dir / \"config.json\"\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        print(f\"   âœ… Saved config: {config_path.name}\")\n",
    "        \n",
    "        # 4. Generate model card\n",
    "        model_card = self._create_model_card(metadata, trainer. history)\n",
    "        card_path = export_dir / \"model_card.md\"\n",
    "        with open(card_path, 'w') as f:\n",
    "            f.write(model_card)\n",
    "        print(f\"   âœ… Saved model card: {card_path.name}\")\n",
    "        \n",
    "        # 5. Copy training plots\n",
    "        plot_src = trainer.save_dir / 'training_progress.png'\n",
    "        if plot_src.exists():\n",
    "            shutil. copy(plot_src, export_dir / 'training_progress. png')\n",
    "            print(f\"   âœ… Saved training plots\")\n",
    "        \n",
    "        # 6. Save training history\n",
    "        history_path = export_dir / \"training_history.json\"\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(trainer.history, f, indent=2)\n",
    "        print(f\"   âœ… Saved training history\")\n",
    "        \n",
    "        print(f\"\\nâœ… Model exported successfully!\")\n",
    "        print(f\"\\nðŸ“‹ Export Summary:\")\n",
    "        print(f\"   Version: {version}\")\n",
    "        print(f\"   Location: {export_dir}\")\n",
    "        print(f\"   Files: {len(list(export_dir.glob('*')))}\")\n",
    "        \n",
    "        return export_dir\n",
    "    \n",
    "    def _create_metadata(self, trainer, config, data_info, version):\n",
    "        \"\"\"Create comprehensive metadata.\"\"\"\n",
    "        metadata = {\n",
    "            \"model_name\": config['model']['name'],\n",
    "            \"version\": version,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"architecture\": {\n",
    "                \"type\": config['model']['architecture'],\n",
    "                \"input_shape\": [1, 512, 512],  # Assuming square images\n",
    "                \"output_channels\": 1\n",
    "            },\n",
    "            \"training\": {\n",
    "                \"epochs\": config['training']['epochs'],\n",
    "                \"batch_size\": config['training']['batch_size'],\n",
    "                \"learning_rate\": config['training']['learning_rate'],\n",
    "                \"optimizer\": \"Adam\",\n",
    "                \"loss_function\": \"BCEWithLogitsLoss\",\n",
    "                \"validation_split\": config['training']['validation_split']\n",
    "            },\n",
    "            \"performance\": {\n",
    "                \"final_train_loss\": trainer.history['train_loss'][-1],\n",
    "                \"final_val_loss\": trainer. history['val_loss'][-1],\n",
    "                \"best_val_loss\": trainer. best_val_loss,\n",
    "                \"final_train_iou\": trainer. history['train_iou'][-1],\n",
    "                \"final_val_iou\": trainer.history['val_iou'][-1],\n",
    "                \"best_val_iou\": max(trainer.history['val_iou'])\n",
    "            },\n",
    "            \"data_info\": data_info,\n",
    "            \"compatibility\": {\n",
    "                \"deeptrack_version\": dt.__version__,\n",
    "                \"deeplay_version\": dl.__version__,\n",
    "                \"torch_version\": torch.__version__,\n",
    "                \"python_version\": f\"{sys.version_info. major}.{sys.version_info.minor}\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if config['model']['architecture'] == 'unet':\n",
    "            metadata['architecture']['unet_channels'] = config['model']['unet_channels']\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def _create_model_card(self, metadata, history):\n",
    "        \"\"\"Generate a human-readable model card in Markdown.\"\"\"\n",
    "        card = f\"\"\"# Model Card:  {metadata['model_name']}\n",
    "\n",
    "**Version:** {metadata['version']}  \n",
    "**Created:** {metadata['created_at']}  \n",
    "**Architecture:** {metadata['architecture']['type']. upper()}\n",
    "\n",
    "## ðŸ“Š Performance\n",
    "\n",
    "| Metric | Train | Validation | Best |\n",
    "|--------|-------|------------|------|\n",
    "| Loss | {metadata['performance']['final_train_loss']:.4f} | {metadata['performance']['final_val_loss']:.4f} | {metadata['performance']['best_val_loss']:.4f} |\n",
    "| IoU | {metadata['performance']['final_train_iou']:.4f} | {metadata['performance']['final_val_iou']:.4f} | {metadata['performance']['best_val_iou']:.4f} |\n",
    "\n",
    "## ðŸ—ï¸ Architecture\n",
    "\n",
    "- **Type:** {metadata['architecture']['type'].upper()}\n",
    "- **Input Shape:** {metadata['architecture']['input_shape']}\n",
    "- **Output Channels:** {metadata['architecture']['output_channels']}\n",
    "\"\"\"\n",
    "        \n",
    "        if 'unet_channels' in metadata['architecture']:\n",
    "            card += f\"- **UNet Channels:** {metadata['architecture']['unet_channels']}\\n\"\n",
    "        \n",
    "        card += f\"\"\"\n",
    "## ðŸŽ¯ Training Configuration\n",
    "\n",
    "- **Epochs:** {metadata['training']['epochs']}\n",
    "- **Batch Size:** {metadata['training']['batch_size']}\n",
    "- **Learning Rate:** {metadata['training']['learning_rate']}\n",
    "- **Optimizer:** {metadata['training']['optimizer']}\n",
    "- **Loss Function:** {metadata['training']['loss_function']}\n",
    "- **Validation Split:** {metadata['training']['validation_split']}\n",
    "\n",
    "## ðŸ“¦ Data Information\n",
    "\n",
    "\"\"\"\n",
    "        for key, value in metadata['data_info'].items():\n",
    "            card += f\"- **{key}:** {value}\\n\"\n",
    "        \n",
    "        card += f\"\"\"\n",
    "## ðŸ”§ Compatibility\n",
    "\n",
    "- **DeepTrack Version:** {metadata['compatibility']['deeptrack_version']}\n",
    "- **PyTorch Version:** {metadata['compatibility']['torch_version']}\n",
    "- **Python Version:** {metadata['compatibility']['python_version']}\n",
    "\n",
    "## ðŸ“ˆ Training Progress\n",
    "\n",
    "![Training Progress](training_progress.png)\n",
    "\n",
    "## ðŸš€ Usage\n",
    "\n",
    "```python\n",
    "# Load model in DeepTrack MPT Studio\n",
    "from src.engines.ai_engine import DeepTrackEngine\n",
    "\n",
    "engine = DeepTrackEngine()\n",
    "engine.load_model(\"path/to/weights.pth\", \"path/to/metadata.json\")\n",
    "predictions = engine.predict(video_stack)\n",
    "```\n",
    "\n",
    "---\n",
    "*Generated by DeepTrack Cloud Trainer*\n",
    "\"\"\"\n",
    "        return card\n",
    "\n",
    "print(\"âœ… Model exporter ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source":  [
    "---\n",
    "## ðŸŽ¬ SECTION 8: Execute Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs":  [],